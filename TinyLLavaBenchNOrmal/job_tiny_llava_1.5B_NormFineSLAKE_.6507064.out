[2024-03-31 15:35:08,252] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 15:35:11,216] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2: setting --include=localhost:0,1,2
[2024-03-31 15:35:11,216] [INFO] [runner.py:568:main] cmd = /home/ltl2113/miniconda3/envs/tinyllava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None tinyllava/train/train.py --deepspeed ./scripts/tiny_llava/zero2.json --model_name_or_path /scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196 --version plain --data_path /scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json --image_folder /scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train --vision_tower google/siglip-so400m-patch14-384 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --tune_mm_mlp_adapter True --tune_entire_model False --fp16 True --output_dir /scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA --num_train_epochs 3 --per_device_train_batch_size 2 --per_device_eval_batch_size 4 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy epoch --learning_rate 2e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 15 --lazy_preprocess True --report_to wandb --run_name tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196-siglip-so400m-patch14-384
[2024-03-31 15:35:13,807] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 15:35:15,606] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2]}
[2024-03-31 15:35:15,606] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=3, node_rank=0
[2024-03-31 15:35:15,606] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2]})
[2024-03-31 15:35:15,606] [INFO] [launch.py:163:main] dist_world_size=3
[2024-03-31 15:35:15,606] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2
[2024-03-31 15:35:15,607] [INFO] [launch.py:253:main] process 318868 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=0', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA', '--num_train_epochs', '3', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196-siglip-so400m-patch14-384']
[2024-03-31 15:35:15,607] [INFO] [launch.py:253:main] process 318869 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=1', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA', '--num_train_epochs', '3', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196-siglip-so400m-patch14-384']
[2024-03-31 15:35:15,612] [INFO] [launch.py:253:main] process 318870 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=2', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA', '--num_train_epochs', '3', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_SG2/checkpoint-2196-siglip-so400m-patch14-384']
[2024-03-31 15:35:19,477] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 15:35:19,477] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 15:35:19,478] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 15:35:20,524] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 15:35:20,524] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 15:35:20,524] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-31 15:35:20,524] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
{'loss': 3.8774, 'learning_rate': 0.0005, 'epoch': 0.03}
{'loss': 3.8428, 'learning_rate': 0.001, 'epoch': 0.05}
{'loss': 3.7697, 'learning_rate': 0.0015, 'epoch': 0.08}
{'loss': 3.8954, 'learning_rate': 0.002, 'epoch': 0.11}
{'loss': 3.9712, 'learning_rate': 0.0019995690062269986, 'epoch': 0.13}
{'loss': 3.8294, 'learning_rate': 0.0019982763964192583, 'epoch': 0.16}
{'loss': 3.8282, 'learning_rate': 0.001996123284790336, 'epoch': 0.19}
{'loss': 3.964, 'learning_rate': 0.0019931115272956404, 'epoch': 0.21}
{'loss': 3.9072, 'learning_rate': 0.0019892437200326237, 'epoch': 0.24}
{'loss': 3.9541, 'learning_rate': 0.0019845231970029775, 'epoch': 0.27}
{'loss': 3.8887, 'learning_rate': 0.0019789540272387627, 'epoch': 0.29}
{'loss': 3.9323, 'learning_rate': 0.001972541011294959, 'epoch': 0.32}
{'loss': 3.9892, 'learning_rate': 0.0019652896771114414, 'epoch': 0.35}
{'loss': 3.953, 'learning_rate': 0.001957206275247968, 'epoch': 0.37}
{'loss': 3.8692, 'learning_rate': 0.0019482977734962752, 'epoch': 0.4}
{'loss': 3.9682, 'learning_rate': 0.001938571850873926, 'epoch': 0.43}
{'loss': 3.8577, 'learning_rate': 0.0019280368910050943, 'epoch': 0.45}
{'loss': 3.9342, 'learning_rate': 0.0019167019748939844, 'epoch': 0.48}
{'loss': 3.8746, 'learning_rate': 0.0019045768730971196, 'epoch': 0.51}
{'loss': 4.0416, 'learning_rate': 0.0018916720373012424, 'epoch': 0.54}
{'loss': 3.8491, 'learning_rate': 0.0018779985913140924, 'epoch': 0.56}
{'loss': 3.7524, 'learning_rate': 0.0018635683214758211, 'epoch': 0.59}
{'loss': 3.8339, 'learning_rate': 0.001848393666499315, 'epoch': 0.62}
{'loss': 3.8702, 'learning_rate': 0.0018324877067481783, 'epoch': 0.64}
{'loss': 3.8512, 'learning_rate': 0.0018158641529616238, 'epoch': 0.67}
{'loss': 3.8779, 'learning_rate': 0.001798537334435986, 'epoch': 0.7}
{'loss': 3.8233, 'learning_rate': 0.0017805221866730458, 'epoch': 0.72}
{'loss': 3.9374, 'learning_rate': 0.0017618342385058146, 'epoch': 0.75}
{'loss': 3.9084, 'learning_rate': 0.001742489598712872, 'epoch': 0.78}
{'loss': 3.8615, 'learning_rate': 0.0017225049421328022, 'epoch': 0.8}
{'loss': 3.8644, 'learning_rate': 0.0017018974952906882, 'epoch': 0.83}
{'loss': 3.8736, 'learning_rate': 0.001680685021549063, 'epoch': 0.86}
{'loss': 3.9152, 'learning_rate': 0.0016588858057961112, 'epoch': 0.88}
{'loss': 3.9445, 'learning_rate': 0.0016365186386843248, 'epoch': 0.91}
{'loss': 3.7368, 'learning_rate': 0.001613602800433194, 'epoch': 0.94}
{'loss': 3.9468, 'learning_rate': 0.0015901580442098967, 'epoch': 0.96}
{'loss': 3.8356, 'learning_rate': 0.0015662045791023172, 'epoch': 0.99}
{'loss': 3.8299, 'learning_rate': 0.0015417630526990613, 'epoch': 1.02}
{'loss': 3.8461, 'learning_rate': 0.001516854533291494, 'epoch': 1.04}
{'loss': 4.0319, 'learning_rate': 0.0014915004917131346, 'epoch': 1.07}
{'loss': 4.0104, 'learning_rate': 0.0014657227828320635, 'epoch': 1.1}
{'loss': 3.8438, 'learning_rate': 0.0014395436267123016, 'epoch': 1.12}
{'loss': 3.8951, 'learning_rate': 0.0014129855894603886, 'epoch': 1.15}
{'loss': 3.8592, 'learning_rate': 0.0013860715637736816, 'epoch': 1.18}
{'loss': 3.8549, 'learning_rate': 0.001358824749207136, 'epoch': 1.2}
{'loss': 3.8726, 'learning_rate': 0.001331268632175576, 'epoch': 1.23}
{'loss': 3.8487, 'learning_rate': 0.0013034269657086993, 'epoch': 1.26}
{'loss': 3.9137, 'learning_rate': 0.00127532374897626, 'epoch': 1.28}
{'loss': 3.9061, 'learning_rate': 0.0012469832066010842, 'epoch': 1.31}
{'loss': 3.8841, 'learning_rate': 0.0012184297677777464, 'epoch': 1.34}
{'loss': 3.9115, 'learning_rate': 0.0011896880452149077, 'epoch': 1.36}
{'loss': 3.9934, 'learning_rate': 0.0011607828139194682, 'epoch': 1.39}
{'loss': 3.9123, 'learning_rate': 0.0011317389898408189, 'epoch': 1.42}
{'loss': 3.9409, 'learning_rate': 0.0011025816083936036, 'epoch': 1.44}
{'loss': 3.853, 'learning_rate': 0.001073335802877504, 'epoch': 1.47}
{'loss': 3.8834, 'learning_rate': 0.0010440267828126478, 'epoch': 1.5}
{'loss': 3.8911, 'learning_rate': 0.0010146798122093166, 'epoch': 1.53}
{'loss': 3.7102, 'learning_rate': 0.0009853201877906836, 'epoch': 1.55}
{'loss': 3.8351, 'learning_rate': 0.0009559732171873523, 'epoch': 1.58}
{'loss': 3.9417, 'learning_rate': 0.0009266641971224963, 'epoch': 1.61}
{'loss': 3.8546, 'learning_rate': 0.0008974183916063967, 'epoch': 1.63}
{'loss': 3.8503, 'learning_rate': 0.0008682610101591814, 'epoch': 1.66}
{'loss': 3.9588, 'learning_rate': 0.0008392171860805319, 'epoch': 1.69}
{'loss': 3.8387, 'learning_rate': 0.0008103119547850923, 'epoch': 1.71}
{'loss': 3.8741, 'learning_rate': 0.0007815702322222538, 'epoch': 1.74}
{'loss': 3.8531, 'learning_rate': 0.000753016793398916, 'epoch': 1.77}
{'loss': 3.8483, 'learning_rate': 0.0007246762510237404, 'epoch': 1.79}
{'loss': 3.8928, 'learning_rate': 0.0006965730342913011, 'epoch': 1.82}
{'loss': 3.9502, 'learning_rate': 0.0006687313678244242, 'epoch': 1.85}
{'loss': 3.8799, 'learning_rate': 0.0006411752507928642, 'epoch': 1.87}
{'loss': 3.8415, 'learning_rate': 0.0006139284362263184, 'epoch': 1.9}
{'loss': 3.9145, 'learning_rate': 0.0005870144105396118, 'epoch': 1.93}
{'loss': 3.925, 'learning_rate': 0.0005604563732876988, 'epoch': 1.95}
{'loss': 3.8642, 'learning_rate': 0.0005342772171679364, 'epoch': 1.98}
{'loss': 3.9694, 'learning_rate': 0.0005084995082868658, 'epoch': 2.01}
{'loss': 3.9009, 'learning_rate': 0.0004831454667085059, 'epoch': 2.03}
{'loss': 3.8676, 'learning_rate': 0.000458236947300939, 'epoch': 2.06}
{'loss': 3.8635, 'learning_rate': 0.000433795420897683, 'epoch': 2.09}
{'loss': 3.8043, 'learning_rate': 0.0004098419557901035, 'epoch': 2.11}
{'loss': 3.9248, 'learning_rate': 0.0003863971995668062, 'epoch': 2.14}
{'loss': 3.9456, 'learning_rate': 0.0003634813613156753, 'epoch': 2.17}
{'loss': 3.883, 'learning_rate': 0.00034111419420388903, 'epoch': 2.19}
{'loss': 3.9044, 'learning_rate': 0.00031931497845093747, 'epoch': 2.22}
{'loss': 3.9435, 'learning_rate': 0.0002981025047093118, 'epoch': 2.25}
{'loss': 3.9216, 'learning_rate': 0.00027749505786719796, 'epoch': 2.27}
{'loss': 3.8698, 'learning_rate': 0.000257510401287128, 'epoch': 2.3}
{'loss': 3.7775, 'learning_rate': 0.00023816576149418578, 'epoch': 2.33}
{'loss': 3.8011, 'learning_rate': 0.00021947781332695406, 'epoch': 2.35}
{'loss': 3.8562, 'learning_rate': 0.00020146266556401405, 'epoch': 2.38}
{'loss': 3.8339, 'learning_rate': 0.00018413584703837616, 'epoch': 2.41}
{'loss': 3.9496, 'learning_rate': 0.00016751229325182195, 'epoch': 2.43}
{'loss': 3.8481, 'learning_rate': 0.0001516063335006851, 'epoch': 2.46}
{'loss': 3.9399, 'learning_rate': 0.00013643167852417894, 'epoch': 2.49}
{'loss': 3.8193, 'learning_rate': 0.00012200140868590758, 'epoch': 2.52}
{'loss': 3.923, 'learning_rate': 0.00010832796269875755, 'epoch': 2.54}
{'loss': 3.8648, 'learning_rate': 9.542312690288036e-05, 'epoch': 2.57}
{'loss': 3.7569, 'learning_rate': 8.329802510601559e-05, 'epoch': 2.6}
{'loss': 3.8625, 'learning_rate': 7.196310899490577e-05, 'epoch': 2.62}
{'loss': 3.8526, 'learning_rate': 6.142814912607409e-05, 'epoch': 2.65}
{'loss': 4.0091, 'learning_rate': 5.170222650372469e-05, 'epoch': 2.68}
{'loss': 3.9647, 'learning_rate': 4.27937247520318e-05, 'epoch': 2.7}
{'loss': 3.8151, 'learning_rate': 3.471032288855869e-05, 'epoch': 2.73}
{'loss': 3.942, 'learning_rate': 2.7458988705041155e-05, 'epoch': 2.76}
{'loss': 3.9077, 'learning_rate': 2.104597276123721e-05, 'epoch': 2.78}
{'loss': 3.8901, 'learning_rate': 1.547680299702281e-05, 'epoch': 2.81}
{'loss': 3.9639, 'learning_rate': 1.075627996737627e-05, 'epoch': 2.84}
{'loss': 3.9512, 'learning_rate': 6.888472704359661e-06, 'epoch': 2.86}
{'loss': 3.8399, 'learning_rate': 3.87671520966415e-06, 'epoch': 2.89}
{'loss': 3.9807, 'learning_rate': 1.7236035807416396e-06, 'epoch': 2.92}
{'loss': 3.9931, 'learning_rate': 4.309937730015978e-07, 'epoch': 2.94}
{'loss': 3.8058, 'learning_rate': 0.0, 'epoch': 2.97}
{'train_runtime': 440.8782, 'train_samples_per_second': 12.207, 'train_steps_per_second': 0.252, 'train_loss': 3.888224002477285, 'epoch': 2.97}
[2024-03-31 15:43:35,165] [INFO] [launch.py:348:main] Process 318870 exits successfully.
[2024-03-31 15:43:35,165] [INFO] [launch.py:348:main] Process 318869 exits successfully.
[2024-03-31 15:43:40,170] [INFO] [launch.py:348:main] Process 318868 exits successfully.
