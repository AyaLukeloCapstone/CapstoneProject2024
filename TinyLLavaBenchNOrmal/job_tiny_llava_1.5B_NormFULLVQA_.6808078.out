[2024-04-07 21:53:18,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 21:53:21,455] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-04-07 21:53:21,456] [INFO] [runner.py:568:main] cmd = /home/ltl2113/miniconda3/envs/tinyllava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None tinyllava/train/train.py --deepspeed ./scripts/tiny_llava/zero2.json --model_name_or_path /scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326 --version plain --data_path /scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json --image_folder /scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train --vision_tower google/siglip-so400m-patch14-384 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --tune_mm_mlp_adapter True --tune_entire_model False --fp16 True --output_dir /scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA_15E_FINAL --num_train_epochs 15 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy epoch --learning_rate 2e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 15 --lazy_preprocess True --report_to wandb --run_name tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326-siglip-so400m-patch14-384
[2024-04-07 21:53:23,369] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 21:53:24,836] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-04-07 21:53:24,836] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-07 21:53:24,836] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-07 21:53:24,836] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-07 21:53:24,836] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-04-07 21:53:24,837] [INFO] [launch.py:253:main] process 1878113 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=0', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA_15E_FINAL', '--num_train_epochs', '15', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326-siglip-so400m-patch14-384']
[2024-04-07 21:53:24,838] [INFO] [launch.py:253:main] process 1878114 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=1', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA_15E_FINAL', '--num_train_epochs', '15', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326-siglip-so400m-patch14-384']
[2024-04-07 21:53:24,838] [INFO] [launch.py:253:main] process 1878115 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=2', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA_15E_FINAL', '--num_train_epochs', '15', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326-siglip-so400m-patch14-384']
[2024-04-07 21:53:24,839] [INFO] [launch.py:253:main] process 1878116 spawned with command: ['/home/ltl2113/miniconda3/envs/tinyllava/bin/python', '-u', 'tinyllava/train/train.py', '--local_rank=3', '--deepspeed', './scripts/tiny_llava/zero2.json', '--model_name_or_path', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326', '--version', 'plain', '--data_path', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train.json', '--image_folder', '/scratch/ltl2113/LLaVA-Med/data/Vqa/VQA-RAD/train', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--tune_mm_mlp_adapter', 'True', '--tune_entire_model', 'False', '--fp16', 'True', '--output_dir', '/scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_VQA_15E_FINAL', '--num_train_epochs', '15', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'epoch', '--learning_rate', '2e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '15', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'tiny-llava-base-pretrain-scratch/ltl2113/LLaVA-Med/TinyLLaVABench/checkpoints/Tinny_llava_Norm_SG2_FULL/checkpoint-1326-siglip-so400m-patch14-384']
[2024-04-07 21:53:28,085] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 21:53:28,095] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 21:53:28,098] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 21:53:28,102] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 21:53:28,968] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-07 21:53:28,968] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-07 21:53:28,968] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-07 21:53:28,968] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-07 21:53:28,968] [INFO] [comm.py:637:init_distributed] cdb=None
{'loss': 3.8323, 'learning_rate': 0.0002857142857142857, 'epoch': 0.07}
{'loss': 3.8978, 'learning_rate': 0.0005714285714285714, 'epoch': 0.14}
{'loss': 3.8661, 'learning_rate': 0.0008571428571428571, 'epoch': 0.21}
{'loss': 3.9016, 'learning_rate': 0.0011428571428571427, 'epoch': 0.28}
{'loss': 3.961, 'learning_rate': 0.0014285714285714286, 'epoch': 0.35}
{'loss': 3.9083, 'learning_rate': 0.0017142857142857142, 'epoch': 0.42}
{'loss': 3.8875, 'learning_rate': 0.002, 'epoch': 0.5}
{'loss': 3.9076, 'learning_rate': 0.001999880251796685, 'epoch': 0.57}
{'loss': 3.8084, 'learning_rate': 0.0019995210358660035, 'epoch': 0.64}
{'loss': 3.8365, 'learning_rate': 0.001998922438238881, 'epoch': 0.71}
{'loss': 3.9099, 'learning_rate': 0.0019980846022772977, 'epoch': 0.78}
{'loss': 3.8511, 'learning_rate': 0.001997007728639956, 'epoch': 0.85}
{'loss': 3.8816, 'learning_rate': 0.0019956920752342224, 'epoch': 0.92}
{'loss': 3.8541, 'learning_rate': 0.0019941379571543597, 'epoch': 0.99}
{'loss': 3.8834, 'learning_rate': 0.0019923457466060634, 'epoch': 1.06}
{'loss': 3.9053, 'learning_rate': 0.0019903158728173205, 'epoch': 1.13}
{'loss': 3.8629, 'learning_rate': 0.0019880488219356086, 'epoch': 1.2}
{'loss': 3.8653, 'learning_rate': 0.001985545136911468, 'epoch': 1.27}
{'loss': 3.8883, 'learning_rate': 0.0019828054173684643, 'epoch': 1.35}
{'loss': 3.9326, 'learning_rate': 0.001979830319459585, 'epoch': 1.42}
{'loss': 3.8792, 'learning_rate': 0.0019766205557100866, 'epoch': 1.49}
{'loss': 3.8205, 'learning_rate': 0.001973176894846855, 'epoch': 1.56}
{'loss': 3.8532, 'learning_rate': 0.0019695001616142913, 'epoch': 1.63}
{'loss': 3.8968, 'learning_rate': 0.0019655912365767942, 'epoch': 1.7}
{'loss': 3.8357, 'learning_rate': 0.0019614510559078624, 'epoch': 1.77}
{'loss': 3.9046, 'learning_rate': 0.00195708061116589, 'epoch': 1.84}
{'loss': 3.8631, 'learning_rate': 0.0019524809490566878, 'epoch': 1.91}
{'loss': 3.9033, 'learning_rate': 0.0019476531711828025, 'epoch': 1.98}
{'loss': 3.9072, 'learning_rate': 0.001942598433779687, 'epoch': 2.05}
{'loss': 3.8396, 'learning_rate': 0.0019373179474387859, 'epoch': 2.12}
{'loss': 3.8939, 'learning_rate': 0.0019318129768176031, 'epoch': 2.19}
{'loss': 3.9086, 'learning_rate': 0.001926084840336821, 'epoch': 2.27}
{'loss': 3.829, 'learning_rate': 0.0019201349098645433, 'epoch': 2.34}
{'loss': 3.8318, 'learning_rate': 0.0019139646103877377, 'epoch': 2.41}
{'loss': 3.8915, 'learning_rate': 0.0019075754196709572, 'epoch': 2.48}
{'loss': 3.8725, 'learning_rate': 0.001900968867902419, 'epoch': 2.55}
{'loss': 3.8121, 'learning_rate': 0.001894146537327533, 'epoch': 2.62}
{'loss': 3.9564, 'learning_rate': 0.0018871100618699553, 'epoch': 2.69}
{'loss': 3.8765, 'learning_rate': 0.0018798611267402746, 'epoch': 2.76}
{'loss': 3.9222, 'learning_rate': 0.0018724014680324057, 'epoch': 2.83}
{'loss': 3.9049, 'learning_rate': 0.001864732872307804, 'epoch': 2.9}
{'loss': 3.8745, 'learning_rate': 0.0018568571761675891, 'epoch': 2.97}
{'loss': 3.9111, 'learning_rate': 0.001848776265812687, 'epoch': 3.04}
{'loss': 3.8273, 'learning_rate': 0.0018404920765920897, 'epoch': 3.12}
{'loss': 3.8785, 'learning_rate': 0.0018320065925393468, 'epoch': 3.19}
{'loss': 3.9143, 'learning_rate': 0.0018233218458973983, 'epoch': 3.26}
{'loss': 3.8577, 'learning_rate': 0.001814439916631857, 'epoch': 3.33}
{'loss': 3.923, 'learning_rate': 0.0018053629319328662, 'epoch': 3.4}
{'loss': 3.9258, 'learning_rate': 0.0017960930657056437, 'epoch': 3.47}
{'loss': 3.804, 'learning_rate': 0.0017866325380498417, 'epoch': 3.54}
{'loss': 3.8743, 'learning_rate': 0.001776983614727838, 'epoch': 3.61}
{'loss': 3.8832, 'learning_rate': 0.0017671486066220966, 'epoch': 3.68}
{'loss': 3.8348, 'learning_rate': 0.0017571298691817178, 'epoch': 3.75}
{'loss': 3.8698, 'learning_rate': 0.0017469298018583169, 'epoch': 3.82}
{'loss': 3.9302, 'learning_rate': 0.001736550847531366, 'epoch': 3.89}
{'loss': 3.8796, 'learning_rate': 0.0017259954919231308, 'epoch': 3.96}
{'loss': 3.8767, 'learning_rate': 0.0017152662630033504, 'epoch': 4.04}
{'loss': 3.9107, 'learning_rate': 0.0017043657303837962, 'epoch': 4.11}
{'loss': 3.8673, 'learning_rate': 0.001693296504702862, 'epoch': 4.18}
{'loss': 3.8532, 'learning_rate': 0.001682061237000322, 'epoch': 4.25}
{'loss': 3.873, 'learning_rate': 0.0016706626180824184, 'epoch': 4.32}
{'loss': 3.8799, 'learning_rate': 0.001659103377877423, 'epoch': 4.39}
{'loss': 3.9277, 'learning_rate': 0.0016473862847818277, 'epoch': 4.46}
{'loss': 3.8774, 'learning_rate': 0.0016355141449973254, 'epoch': 4.53}
{'loss': 3.9133, 'learning_rate': 0.0016234898018587336, 'epoch': 4.6}
{'loss': 3.891, 'learning_rate': 0.001611316135153026, 'epoch': 4.67}
{'loss': 3.8541, 'learning_rate': 0.0015989960604296338, 'epoch': 4.74}
{'loss': 3.8645, 'learning_rate': 0.001586532528302183, 'epoch': 4.81}
{'loss': 3.8537, 'learning_rate': 0.001573928523741832, 'epoch': 4.88}
{'loss': 3.8919, 'learning_rate': 0.0015611870653623825, 'epoch': 4.96}
{'loss': 3.9012, 'learning_rate': 0.0015483112046973308, 'epoch': 5.03}
{'loss': 3.877, 'learning_rate': 0.0015353040254690393, 'epoch': 5.1}
{'loss': 3.895, 'learning_rate': 0.001522168642850193, 'epoch': 5.17}
{'loss': 3.8558, 'learning_rate': 0.001508908202717729, 'epoch': 5.24}
{'loss': 3.8709, 'learning_rate': 0.0014955258808994093, 'epoch': 5.31}
{'loss': 3.8399, 'learning_rate': 0.001482024882413222, 'epoch': 5.38}
{'loss': 3.8585, 'learning_rate': 0.0014684084406997903, 'epoch': 5.45}
{'loss': 3.9238, 'learning_rate': 0.0014546798168479755, 'epoch': 5.52}
{'loss': 3.8749, 'learning_rate': 0.0014408422988138583, 'epoch': 5.59}
{'loss': 3.8616, 'learning_rate': 0.0014268992006332846, 'epoch': 5.66}
{'loss': 3.8599, 'learning_rate': 0.0014128538616281659, 'epoch': 5.73}
{'loss': 3.9447, 'learning_rate': 0.0013987096456067234, 'epoch': 5.81}
{'loss': 3.8647, 'learning_rate': 0.0013844699400578696, 'epoch': 5.88}
{'loss': 3.8867, 'learning_rate': 0.0013701381553399147, 'epoch': 5.95}
{'loss': 3.9545, 'learning_rate': 0.0013557177238637985, 'epoch': 6.02}
{'loss': 3.8259, 'learning_rate': 0.0013412120992710424, 'epoch': 6.09}
{'loss': 3.8654, 'learning_rate': 0.0013266247556066122, 'epoch': 6.16}
{'loss': 3.9685, 'learning_rate': 0.0013119591864868979, 'epoch': 6.23}
{'loss': 3.8829, 'learning_rate': 0.0012972189042630043, 'epoch': 6.3}
{'loss': 3.785, 'learning_rate': 0.001282407439179557, 'epoch': 6.37}
{'loss': 3.8512, 'learning_rate': 0.0012675283385292211, 'epoch': 6.44}
{'loss': 3.9059, 'learning_rate': 0.0012525851658031352, 'epoch': 6.51}
{'loss': 3.9163, 'learning_rate': 0.0012375814998374713, 'epoch': 6.58}
{'loss': 3.9246, 'learning_rate': 0.0012225209339563144, 'epoch': 6.65}
{'loss': 3.8254, 'learning_rate': 0.001207407075111075, 'epoch': 6.73}
{'loss': 3.8857, 'learning_rate': 0.001192243543016637, 'epoch': 6.8}
{'loss': 3.8854, 'learning_rate': 0.0011770339692844483, 'epoch': 6.87}
{'loss': 3.8909, 'learning_rate': 0.001161781996552765, 'epoch': 6.94}
{'loss': 3.8943, 'learning_rate': 0.0011464912776142492, 'epoch': 7.01}
{'loss': 3.8632, 'learning_rate': 0.0011311654745411424, 'epoch': 7.08}
{'loss': 3.8777, 'learning_rate': 0.0011158082578082088, 'epoch': 7.15}
{'loss': 3.8257, 'learning_rate': 0.0011004233054136725, 'epoch': 7.22}
{'loss': 3.8711, 'learning_rate': 0.0010850143019983474, 'epoch': 7.29}
{'loss': 3.8648, 'learning_rate': 0.0010695849379631812, 'epoch': 7.36}
{'loss': 3.925, 'learning_rate': 0.0010541389085854177, 'epoch': 7.43}
{'loss': 3.9263, 'learning_rate': 0.001038679913133589, 'epoch': 7.5}
{'loss': 3.9148, 'learning_rate': 0.0010232116539815558, 'epoch': 7.58}
{'loss': 3.9067, 'learning_rate': 0.0010077378357218022, 'epoch': 7.65}
{'loss': 3.8891, 'learning_rate': 0.0009922621642781979, 'epoch': 7.72}
{'loss': 3.8592, 'learning_rate': 0.0009767883460184443, 'epoch': 7.79}
{'loss': 3.899, 'learning_rate': 0.0009613200868664112, 'epoch': 7.86}
{'loss': 3.8334, 'learning_rate': 0.0009458610914145825, 'epoch': 7.93}
{'loss': 3.8625, 'learning_rate': 0.0009304150620368188, 'epoch': 8.0}
{'loss': 3.9322, 'learning_rate': 0.0009149856980016529, 'epoch': 8.07}
{'loss': 3.884, 'learning_rate': 0.0008995766945863276, 'epoch': 8.14}
{'loss': 3.891, 'learning_rate': 0.0008841917421917912, 'epoch': 8.21}
{'loss': 3.8533, 'learning_rate': 0.0008688345254588578, 'epoch': 8.28}
{'loss': 3.8454, 'learning_rate': 0.0008535087223857508, 'epoch': 8.35}
{'loss': 3.9244, 'learning_rate': 0.0008382180034472353, 'epoch': 8.42}
{'loss': 3.8217, 'learning_rate': 0.0008229660307155518, 'epoch': 8.5}
{'loss': 3.8885, 'learning_rate': 0.0008077564569833632, 'epoch': 8.57}
{'loss': 3.8764, 'learning_rate': 0.0007925929248889249, 'epoch': 8.64}
{'loss': 3.8747, 'learning_rate': 0.0007774790660436857, 'epoch': 8.71}
{'loss': 3.8403, 'learning_rate': 0.0007624185001625291, 'epoch': 8.78}
{'loss': 3.9276, 'learning_rate': 0.0007474148341968652, 'epoch': 8.85}
{'loss': 3.9141, 'learning_rate': 0.0007324716614707794, 'epoch': 8.92}
{'loss': 3.8558, 'learning_rate': 0.0007175925608204428, 'epoch': 8.99}
{'loss': 3.8755, 'learning_rate': 0.0007027810957369957, 'epoch': 9.06}
{'loss': 3.8694, 'learning_rate': 0.0006880408135131023, 'epoch': 9.13}
{'loss': 3.9076, 'learning_rate': 0.0006733752443933878, 'epoch': 9.2}
{'loss': 3.8607, 'learning_rate': 0.0006587879007289575, 'epoch': 9.27}
{'loss': 3.8545, 'learning_rate': 0.0006442822761362015, 'epoch': 9.35}
{'loss': 3.8545, 'learning_rate': 0.0006298618446600856, 'epoch': 9.42}
{'loss': 3.8351, 'learning_rate': 0.0006155300599421306, 'epoch': 9.49}
{'loss': 3.8974, 'learning_rate': 0.0006012903543932766, 'epoch': 9.56}
{'loss': 3.8816, 'learning_rate': 0.0005871461383718344, 'epoch': 9.63}
{'loss': 3.8543, 'learning_rate': 0.0005731007993667154, 'epoch': 9.7}
{'loss': 3.8982, 'learning_rate': 0.0005591577011861419, 'epoch': 9.77}
{'loss': 3.9558, 'learning_rate': 0.0005453201831520244, 'epoch': 9.84}
{'loss': 3.898, 'learning_rate': 0.00053159155930021, 'epoch': 9.91}
{'loss': 3.8905, 'learning_rate': 0.0005179751175867784, 'epoch': 9.98}
{'loss': 3.8445, 'learning_rate': 0.0005044741191005908, 'epoch': 10.05}
{'loss': 3.8518, 'learning_rate': 0.0004910917972822713, 'epoch': 10.12}
{'loss': 3.8892, 'learning_rate': 0.0004778313571498074, 'epoch': 10.19}
{'loss': 3.8844, 'learning_rate': 0.0004646959745309609, 'epoch': 10.27}
{'loss': 3.9494, 'learning_rate': 0.00045168879530266905, 'epoch': 10.34}
{'loss': 3.9188, 'learning_rate': 0.00043881293463761775, 'epoch': 10.41}
{'loss': 3.8797, 'learning_rate': 0.00042607147625816765, 'epoch': 10.48}
{'loss': 3.8508, 'learning_rate': 0.00041346747169781697, 'epoch': 10.55}
{'loss': 3.8463, 'learning_rate': 0.0004010039395703664, 'epoch': 10.62}
{'loss': 3.8852, 'learning_rate': 0.00038868386484697417, 'epoch': 10.69}
{'loss': 3.8382, 'learning_rate': 0.0003765101981412665, 'epoch': 10.76}
{'loss': 3.8783, 'learning_rate': 0.00036448585500267484, 'epoch': 10.83}
{'loss': 3.9228, 'learning_rate': 0.00035261371521817244, 'epoch': 10.9}
{'loss': 3.8486, 'learning_rate': 0.00034089662212257734, 'epoch': 10.97}
{'loss': 3.9157, 'learning_rate': 0.00032933738191758157, 'epoch': 11.04}
{'loss': 3.8471, 'learning_rate': 0.0003179387629996782, 'epoch': 11.12}
{'loss': 3.8695, 'learning_rate': 0.0003067034952971381, 'epoch': 11.19}
{'loss': 3.8961, 'learning_rate': 0.0002956342696162037, 'epoch': 11.26}
{'loss': 3.9375, 'learning_rate': 0.00028473373699665, 'epoch': 11.33}
{'loss': 3.8659, 'learning_rate': 0.00027400450807686937, 'epoch': 11.4}
{'loss': 3.8546, 'learning_rate': 0.0002634491524686341, 'epoch': 11.47}
{'loss': 3.9248, 'learning_rate': 0.00025307019814168344, 'epoch': 11.54}
{'loss': 3.8171, 'learning_rate': 0.00024287013081828257, 'epoch': 11.61}
{'loss': 3.8584, 'learning_rate': 0.0002328513933779034, 'epoch': 11.68}
{'loss': 3.8441, 'learning_rate': 0.00022301638527216195, 'epoch': 11.75}
{'loss': 3.8921, 'learning_rate': 0.00021336746195015844, 'epoch': 11.82}
{'loss': 3.884, 'learning_rate': 0.00020390693429435625, 'epoch': 11.89}
{'loss': 3.9083, 'learning_rate': 0.0001946370680671341, 'epoch': 11.96}
{'loss': 3.8916, 'learning_rate': 0.00018556008336814302, 'epoch': 12.04}
{'loss': 3.9389, 'learning_rate': 0.0001766781541026018, 'epoch': 12.11}
{'loss': 3.9039, 'learning_rate': 0.0001679934074606533, 'epoch': 12.18}
{'loss': 3.9552, 'learning_rate': 0.00015950792340791041, 'epoch': 12.25}
{'loss': 3.8866, 'learning_rate': 0.00015122373418731305, 'epoch': 12.32}
{'loss': 3.8866, 'learning_rate': 0.00014314282383241095, 'epoch': 12.39}
{'loss': 3.8998, 'learning_rate': 0.00013526712769219618, 'epoch': 12.46}
{'loss': 3.859, 'learning_rate': 0.00012759853196759452, 'epoch': 12.53}
{'loss': 3.9064, 'learning_rate': 0.0001201388732597255, 'epoch': 12.6}
{'loss': 3.9256, 'learning_rate': 0.00011288993813004467, 'epoch': 12.67}
{'loss': 3.8129, 'learning_rate': 0.00010585346267246743, 'epoch': 12.74}
{'loss': 3.8674, 'learning_rate': 9.903113209758097e-05, 'epoch': 12.81}
{'loss': 3.7645, 'learning_rate': 9.24245803290431e-05, 'epoch': 12.88}
{'loss': 3.8714, 'learning_rate': 8.603538961226232e-05, 'epoch': 12.96}
{'loss': 3.8896, 'learning_rate': 7.986509013545672e-05, 'epoch': 13.03}
{'loss': 3.8574, 'learning_rate': 7.391515966317908e-05, 'epoch': 13.1}
{'loss': 3.9048, 'learning_rate': 6.81870231823969e-05, 'epoch': 13.17}
{'loss': 3.7773, 'learning_rate': 6.268205256121395e-05, 'epoch': 13.24}
{'loss': 3.8712, 'learning_rate': 5.740156622031301e-05, 'epoch': 13.31}
{'loss': 3.8431, 'learning_rate': 5.234682881719766e-05, 'epoch': 13.38}
{'loss': 3.8673, 'learning_rate': 4.751905094331232e-05, 'epoch': 13.45}
{'loss': 3.857, 'learning_rate': 4.2919388834110065e-05, 'epoch': 13.52}
{'loss': 3.9018, 'learning_rate': 3.854894409213761e-05, 'epoch': 13.59}
{'loss': 3.8495, 'learning_rate': 3.4408763423206094e-05, 'epoch': 13.66}
{'loss': 3.9431, 'learning_rate': 3.0499838385708578e-05, 'epoch': 13.73}
{'loss': 3.8615, 'learning_rate': 2.6823105153145123e-05, 'epoch': 13.81}
{'loss': 3.9048, 'learning_rate': 2.3379444289913342e-05, 'epoch': 13.88}
{'loss': 3.9223, 'learning_rate': 2.016968054041546e-05, 'epoch': 13.95}
{'loss': 3.9472, 'learning_rate': 1.7194582631535616e-05, 'epoch': 14.02}
{'loss': 3.8927, 'learning_rate': 1.4454863088532388e-05, 'epoch': 14.09}
{'loss': 3.8916, 'learning_rate': 1.1951178064391499e-05, 'epoch': 14.16}
{'loss': 3.8781, 'learning_rate': 9.684127182679524e-06, 'epoch': 14.23}
{'loss': 3.9969, 'learning_rate': 7.654253393936438e-06, 'epoch': 14.3}
{'loss': 3.9238, 'learning_rate': 5.862042845640403e-06, 'epoch': 14.37}
{'loss': 3.8763, 'learning_rate': 4.307924765777682e-06, 'epoch': 14.44}
{'loss': 3.8443, 'learning_rate': 2.9922713600439856e-06, 'epoch': 14.51}
{'loss': 3.8624, 'learning_rate': 1.915397722702217e-06, 'epoch': 14.58}
{'loss': 3.8381, 'learning_rate': 1.0775617611189503e-06, 'epoch': 14.65}
{'loss': 3.8416, 'learning_rate': 4.789641339963957e-07, 'epoch': 14.73}
{'loss': 3.814, 'learning_rate': 1.1974820331517311e-07, 'epoch': 14.8}
{'loss': 3.8477, 'learning_rate': 0.0, 'epoch': 14.87}
{'train_runtime': 1148.2002, 'train_samples_per_second': 23.437, 'train_steps_per_second': 0.183, 'train_loss': 3.8800103755224318, 'epoch': 14.87}
[2024-04-07 22:13:38,162] [INFO] [launch.py:348:main] Process 1878115 exits successfully.
[2024-04-07 22:13:39,163] [INFO] [launch.py:348:main] Process 1878114 exits successfully.
[2024-04-07 22:13:39,163] [INFO] [launch.py:348:main] Process 1878116 exits successfully.
[2024-04-07 22:13:44,168] [INFO] [launch.py:348:main] Process 1878113 exits successfully.
